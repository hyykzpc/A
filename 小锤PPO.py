import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback


# âœ… è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° + æå‰ç»ˆæ­¢æœºåˆ¶
def custom_reward(state):
    x, x_dot, theta, theta_dot = state
    if abs(theta) > 0.26 or abs(x) > 2.4:
        return -100, True  # æå‰ç»ˆæ­¢ï¼ˆè§’åº¦æˆ–ä½ç½®è¶…è¿‡é˜ˆå€¼ï¼‰

    # å¥–åŠ±è®¡ç®—ï¼šé¼“åŠ±stay aliveï¼ŒåŒæ—¶æƒ©ç½šè§’åº¦å’Œä½ç½®åç§»
    penalty = (abs(theta) / 0.26)**2 + 0.2 * abs(theta_dot) + 0.6 * (abs(x) / 2.4)**2
    reward = 1.0 - penalty
    return reward, False


# âœ… å°è£…çŽ¯å¢ƒ
class CustomRewardEnv(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        obs = np.array(obs)
        reward, custom_done = custom_reward(obs)

        # å¼ºåˆ¶æå‰ç»ˆæ­¢
        if custom_done:
            terminated = True

        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        return np.array(obs), info


# âœ… è®°å½•å¥–åŠ±ç”¨ Callback
class RewardLogger(BaseCallback):
    def __init__(self):
        super().__init__()
        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        dones = self.locals.get("dones")
        infos = self.locals.get("infos")

        if dones is not None:
            for i, done in enumerate(dones):
                if done:
                    info = infos[i]
                    if "episode" in info:
                        self.episode_rewards.append(info["episode"]["r"])
                        self.episode_lengths.append(info["episode"]["l"])
        return True


def moving_average(data, window_size=10):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')


def train_ppo(total_timesteps=100000):
    env = CustomRewardEnv(gym.make("CartPole-v1"))
    model = PPO("MlpPolicy", env, verbose=1)
    reward_logger = RewardLogger()
    model.learn(total_timesteps=total_timesteps, callback=reward_logger)
    return model, reward_logger


def test_model(model, episodes=10):
    env = CustomRewardEnv(gym.make("CartPole-v1"))
    steps_list = []
    rewards_list = []

    print(f"\nå¼€å§‹æµ‹è¯•ï¼Œå…±è¿›è¡Œ {episodes} æ¬¡è¯„ä¼°...\n")

    for ep in range(1, episodes + 1):
        print(f"\n>>> ç¬¬ {ep} æ¬¡æµ‹è¯•å¼€å§‹")  # â† ç¡®è®¤æ˜¯å¦å¡åœ¨æŸæ¬¡
        obs, info = env.reset()
        done = False
        step = 0
        total_reward = 0.0

        try:
            while not done:
                action, _ = model.predict(obs, deterministic=True)

                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                total_reward += reward
                step += 1

                if step % 100 == 0:
                    print(f"    ç¬¬ {ep} æ¬¡æµ‹è¯•è¿›è¡Œä¸­ï¼Œå½“å‰æ­¥æ•°: {step}")

                if step > 2000:  # å¼ºåˆ¶ç»ˆæ­¢é˜²æ­¢å¡ä½
                    print(f"âš ï¸ ç¬¬ {ep} æ¬¡æµ‹è¯•è¶…è¿‡ 2000 æ­¥ï¼Œå¯èƒ½å¡ä½ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    break

        except Exception as e:
            print(f"âŒ ç¬¬ {ep} æ¬¡æµ‹è¯•å‘ç”Ÿå¼‚å¸¸ï¼š{e}")
            break

        steps_list.append(step)
        rewards_list.append(total_reward)
        print(f"âœ… ç¬¬ {ep} æ¬¡æµ‹è¯•å®Œæˆï¼šæ­¥æ•° = {step}, æ€»å¥–åŠ± = {total_reward:.3f}")

    env.close()

    print(f"\nðŸ”š å…±å®Œæˆ {len(steps_list)} æ¬¡æµ‹è¯•")
    if len(steps_list) > 0:
        avg_steps = np.mean(steps_list)
        avg_reward = np.mean(rewards_list)
        print(f"ðŸ“Š å¹³å‡æ­¥æ•°ï¼š{avg_steps:.2f}ï¼Œå¹³å‡å¥–åŠ±ï¼š{avg_reward:.3f}")




if __name__ == "__main__":
    model, reward_logger = train_ppo(total_timesteps=10000)
    test_model(model, episodes=10)
    rewards = reward_logger.episode_rewards
    if len(rewards) >= 10:
        avg_rewards = moving_average(rewards, window_size=10)
        plt.plot(avg_rewards)
        plt.xlabel("Episode")
        plt.ylabel("Smoothed Reward (window=10)")
        plt.title("Smoothed Training Reward Curve")
        plt.grid(True)

    else:
        print("Not enough episodes to compute moving average.")

    test_model(model, episodes=10)
